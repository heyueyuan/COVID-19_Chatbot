{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pIx1U2rwz-7_"
   },
   "source": [
    "<center><h1>Covid-19 Chatbot</h1></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oefZpzWQ0F63"
   },
   "source": [
    "## Import Package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "Pg8OCR2LNDud"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "import re\n",
    "import os\n",
    "import unicodedata\n",
    "import itertools\n",
    "\n",
    "cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if cuda else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XYvF_t8g0Uw1"
   },
   "source": [
    "## Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 419
    },
    "id": "hmdQdH4RSmlw",
    "outputId": "331da54a-106a-4350-fa93-db3ed5e9950b"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_excel('WHO_FAQ.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Caculate how many words in answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Length']= data['Answer'].apply(lambda x: len(x.split(' ')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Context</th>\n",
       "      <th>Answer</th>\n",
       "      <th>Length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is a coronavirus?</td>\n",
       "      <td>Coronaviruses are a large family of viruses wh...</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What is a coronavirus?</td>\n",
       "      <td>In humans, several coronaviruses are known to ...</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What is COVID-19?</td>\n",
       "      <td>COVID-19 is the infectious disease caused by t...</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What are the symptoms of COVID-19?</td>\n",
       "      <td>The most common symptoms of COVID-19 are fever...</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What are the symptoms of COVID-19?</td>\n",
       "      <td>Some people become infected but don’t develop ...</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>Are smokers and tobacco users at higher risk o...</td>\n",
       "      <td>Smokers are likely to be more vulnerable to CO...</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>Are smokers and tobacco users at higher risk o...</td>\n",
       "      <td>Smoking products such as water pipes often inv...</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>How large does a meeting or event need to be i...</td>\n",
       "      <td>High profile international sporting events suc...</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>How large does a meeting or event need to be i...</td>\n",
       "      <td>An event counts as a “mass gatherings” if the ...</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>Does WHO recommend that all international mass...</td>\n",
       "      <td>No. As each international mass gathering is di...</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>85 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Context  \\\n",
       "0                              What is a coronavirus?   \n",
       "1                              What is a coronavirus?   \n",
       "2                                   What is COVID-19?   \n",
       "3                  What are the symptoms of COVID-19?   \n",
       "4                  What are the symptoms of COVID-19?   \n",
       "..                                                ...   \n",
       "80  Are smokers and tobacco users at higher risk o...   \n",
       "81  Are smokers and tobacco users at higher risk o...   \n",
       "82  How large does a meeting or event need to be i...   \n",
       "83  How large does a meeting or event need to be i...   \n",
       "84  Does WHO recommend that all international mass...   \n",
       "\n",
       "                                               Answer  Length  \n",
       "0   Coronaviruses are a large family of viruses wh...      15  \n",
       "1   In humans, several coronaviruses are known to ...      19  \n",
       "2   COVID-19 is the infectious disease caused by t...      29  \n",
       "3   The most common symptoms of COVID-19 are fever...      35  \n",
       "4   Some people become infected but don’t develop ...      25  \n",
       "..                                                ...     ...  \n",
       "80  Smokers are likely to be more vulnerable to CO...      38  \n",
       "81  Smoking products such as water pipes often inv...      27  \n",
       "82  High profile international sporting events suc...      26  \n",
       "83  An event counts as a “mass gatherings” if the ...      41  \n",
       "84  No. As each international mass gathering is di...      23  \n",
       "\n",
       "[85 rows x 3 columns]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zLT_pr9j0aoO"
   },
   "source": [
    "### Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "id": "o0K6TllqOooU"
   },
   "outputs": [],
   "source": [
    "MAX_LENGTH = 100  \n",
    "\n",
    "# def unicodeToAscii(s):\n",
    "#     return ''.join(\n",
    "#         c for c in unicodedata.normalize('NFD', s)\n",
    "#         if unicodedata.category(c) != 'Mn'\n",
    "#     )\n",
    " \n",
    "def normalize(s):\n",
    "#     s = unicodeToAscii(s.lower().strip())\n",
    "    s = s.lower().strip()\n",
    "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
    "    s = re.sub(r\"\\s+\", r\" \", s).strip()\n",
    "    return s\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First make all words lower and remove leading and trailing spaces, add a space before punctuations in order to treat punctuation as a word. Next, make string which is not a word or a punctuation into space. Last, replace multiple spaces into one space and remove the leading and trailing spaces again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "id": "FmJ0_G44TusX"
   },
   "outputs": [],
   "source": [
    "pairs = [[data['Context'].apply(normalize).map(str)[i],\n",
    "         data['Answer'].apply(normalize).map(str)[i]] for i in range(data.shape[0])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eK5BBoCc0ojh"
   },
   "source": [
    "### Build Vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This step is like a tokenizer, so define a class, which will save the mapping from words to indexes, and also reverse mapping from indexes to words. In addition, it also records the number of occurences of each word and the total number of words that appear. This class provides the addWord method to add a word, the addSentence method to add sentences, and the method trim to remove low-frequency words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "id": "j2iVdQ1lOk2b"
   },
   "outputs": [],
   "source": [
    "PAD_token = 0  # padding \n",
    "SOS_token = 1  # start of sentence token\n",
    "EOS_token = 2  # end of sentence token\n",
    "\n",
    "class Voc:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.trimmed = False\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {PAD_token: \"PAD\", SOS_token: \"SOS\", EOS_token: \"EOS\"}\n",
    "        self.num_words = 3  \n",
    "\n",
    "    def addSentence(self, sentence):\n",
    "        for word in sentence.split(' '):\n",
    "            self.addWord(word)\n",
    "\n",
    "    def addWord(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.num_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.num_words] = word\n",
    "            self.num_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1\n",
    "\n",
    "    # remove token below min_count\n",
    "    def trim(self, min_count):\n",
    "        if self.trimmed:\n",
    "            return\n",
    "        self.trimmed = True\n",
    "\n",
    "        keep_words = []\n",
    "\n",
    "        for k, v in self.word2count.items():\n",
    "            if v >= min_count:\n",
    "                keep_words.append(k)\n",
    "\n",
    "        print('keep_words {} / {} = {:.4f}'.format(\n",
    "            len(keep_words), len(self.word2index), len(keep_words) / len(self.word2index)\n",
    "        ))\n",
    "\n",
    "        # Reinitialize dictionaries\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {PAD_token: \"PAD\", SOS_token: \"SOS\", EOS_token: \"EOS\"}\n",
    "        self.num_words = 3 \n",
    "        \n",
    "        for word in keep_words:\n",
    "            self.addWord(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we don't have a lot of data, so we don't filter sentences which are very long."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def filterPair(p): \n",
    "#     return len(p[0].split(' ')) < MAX_LENGTH and len(p[1].split(' ')) < MAX_LENGTH\n",
    "\n",
    "# def filterPairs(pairs):\n",
    "#     return [pair for pair in pairs if filterPair(pair)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counted words: 699\n"
     ]
    }
   ],
   "source": [
    "corpus_name = 'covid-vocabulary'\n",
    "voc = Voc(corpus_name)\n",
    "# pairs = filterPairs(pairs)\n",
    "for pair in pairs:\n",
    "    voc.addSentence(pair[0])\n",
    "    voc.addSentence(pair[1])\n",
    "print(\"Counted words:\", voc.num_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ybU5ujiNXSLW",
    "outputId": "48736755-27aa-43e1-c147-604430e63be5",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pairs:\n",
      "['what is a coronavirus ?', 'coronaviruses are a large family of viruses which may cause illness in animals or humans .']\n",
      "['what is a coronavirus ?', 'in humans several coronaviruses are known to cause respiratory infections ranging from the common cold to more severe diseases']\n",
      "['what is covid ?', 'covid is the infectious disease caused by the most recently discovered coronavirus . this new virus and disease were unknown before the outbreak began in wuhan china in december .']\n"
     ]
    }
   ],
   "source": [
    "print(\"pairs:\")\n",
    "for pair in pairs[:3]:\n",
    "    print(pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "pair_batch = [random.choice(pairs) for _ in range(batch_size)]\n",
    "pair_batch.sort(key=lambda x: len(x[0].split(\" \")), reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def indexesFromSentence(voc, sentence):\n",
    "    return [voc.word2index[word] for word in sentence.split(' ')] + [EOS_token]\n",
    "\n",
    "def binaryMatrix(l, value=PAD_token):\n",
    "    m = []\n",
    "    for i, seq in enumerate(l):\n",
    "        m.append([])\n",
    "        for token in seq:\n",
    "            if token == PAD_token:\n",
    "                m[i].append(0)\n",
    "            else:\n",
    "                m[i].append(1)\n",
    "    return m\n",
    "\n",
    "def zeroPadding(l, fillvalue=PAD_token):\n",
    "    return list(itertools.zip_longest(*l, fillvalue=fillvalue))\n",
    "\n",
    "def inputVar(batch, voc):\n",
    "    indexes_batch = [indexesFromSentence(voc, sentence) for sentence in batch]\n",
    "    lengths = torch.tensor([len(indexes) for indexes in indexes_batch])\n",
    "    padList = zeroPadding(indexes_batch)\n",
    "    padVar = torch.LongTensor(padList)\n",
    "    return padVar, lengths\n",
    "\n",
    "def outputVar(batch, voc):\n",
    "    indexes_batch = [indexesFromSentence(voc, sentence) for sentence in batch]\n",
    "    max_target_len = max([len(indexes) for indexes in indexes_batch])\n",
    "    padList = zeroPadding(indexes_batch)\n",
    "    mask = binaryMatrix(padList)\n",
    "    mask = torch.ByteTensor(mask)\n",
    "    padVar = torch.LongTensor(padList)\n",
    "    return padVar, mask, max_target_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_batch, output_batch = [], []\n",
    "for pair in pair_batch:\n",
    "    input_batch.append(pair[0])\n",
    "    output_batch.append(pair[1])\n",
    "input_variable, lengths = inputVar(input_batch, voc)\n",
    "target_variable, mask, max_target_len = outputVar(output_batch, voc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M7FcQVFj1LQ2"
   },
   "source": [
    "## Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "id": "-5PF-EeSPEBg"
   },
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, embedding, n_layers=1, dropout=0):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = embedding\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, n_layers,\n",
    "                          dropout=(0 if n_layers == 1 else dropout), bidirectional=True)\n",
    "\n",
    "    def forward(self, input_seq, input_lengths, hidden=None):\n",
    "        embedded = self.embedding(input_seq)\n",
    "        packed = torch.nn.utils.rnn.pack_padded_sequence(embedded, input_lengths.cpu())\n",
    "        outputs, hidden = self.gru(packed, hidden)\n",
    "        outputs, _ = torch.nn.utils.rnn.pad_packed_sequence(outputs)\n",
    "        outputs = outputs[:, :, :self.hidden_size] + outputs[:, : ,self.hidden_size:]\n",
    "\n",
    "        return outputs, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "id": "mZBNiDMaPJCz"
   },
   "outputs": [],
   "source": [
    "# Luong attention layer\n",
    "class Attn(torch.nn.Module):\n",
    "    def __init__(self, method, hidden_size):\n",
    "        super(Attn, self).__init__()\n",
    "        self.method = method\n",
    "        if self.method not in ['dot', 'general', 'concat']:\n",
    "            raise ValueError(self.method, \"is not an appropriate attention method.\")\n",
    "        self.hidden_size = hidden_size\n",
    "        if self.method == 'general':\n",
    "            self.attn = torch.nn.Linear(self.hidden_size, hidden_size)\n",
    "        elif self.method == 'concat':\n",
    "            self.attn = torch.nn.Linear(self.hidden_size * 2, hidden_size)\n",
    "            self.v = torch.nn.Parameter(torch.FloatTensor(hidden_size))\n",
    "\n",
    "    def dot_score(self, hidden, encoder_output):\n",
    "        return torch.sum(hidden * encoder_output, dim=2)\n",
    "\n",
    "    def general_score(self, hidden, encoder_output):\n",
    "        energy = self.attn(encoder_output)\n",
    "        return torch.sum(hidden * energy, dim=2)\n",
    "\n",
    "    def concat_score(self, hidden, encoder_output):\n",
    "        energy = self.attn(torch.cat((hidden.expand(encoder_output.size(0), -1, -1), encoder_output), 2)).tanh()\n",
    "        return torch.sum(self.v * energy, dim=2)\n",
    "    \n",
    "    def forward(self, hidden, encoder_outputs):\n",
    "        if self.method == 'general':\n",
    "            attn_energies = self.general_score(hidden, encoder_outputs)\n",
    "        elif self.method == 'concat':\n",
    "            attn_energies = self.concat_score(hidden, encoder_outputs)\n",
    "        elif self.method == 'dot':\n",
    "            attn_energies = self.dot_score(hidden, encoder_outputs)\n",
    "\n",
    "        attn_energies = attn_energies.t()\n",
    "\n",
    "        return F.softmax(attn_energies, dim=1).unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "id": "BB6fZsAoPLbM"
   },
   "outputs": [],
   "source": [
    "class LuongAttnDecoderRNN(nn.Module):\n",
    "    def __init__(self, attn_model, embedding, hidden_size, output_size, n_layers=1, dropout=0.1):\n",
    "        super(LuongAttnDecoderRNN, self).__init__()\n",
    "\n",
    "        # Keep for reference\n",
    "        self.attn_model = attn_model\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.dropout = dropout\n",
    "\n",
    "        # Define layers\n",
    "        self.embedding = embedding\n",
    "        self.embedding_dropout = nn.Dropout(dropout)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, n_layers, dropout=(0 if n_layers == 1 else dropout))\n",
    "        self.concat = nn.Linear(hidden_size * 2, hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "        self.attn = Attn(attn_model, hidden_size)\n",
    "\n",
    "    def forward(self, input_step, last_hidden, encoder_outputs):\n",
    "        # Note: we run this one step (word) at a time\n",
    "        # Get embedding of current input word\n",
    "        embedded = self.embedding(input_step)\n",
    "        embedded = self.embedding_dropout(embedded)\n",
    "        # Forward through unidirectional GRU\n",
    "        rnn_output, hidden = self.gru(embedded, last_hidden)\n",
    "        # Calculate attention weights from the current GRU output\n",
    "        attn_weights = self.attn(rnn_output, encoder_outputs)\n",
    "        # Multiply attention weights to encoder outputs to get new \"weighted sum\" context vector\n",
    "        context = attn_weights.bmm(encoder_outputs.transpose(0, 1))\n",
    "        # Concatenate weighted context vector and GRU output using Luong eq. 5\n",
    "        rnn_output = rnn_output.squeeze(0)\n",
    "        context = context.squeeze(1)\n",
    "        concat_input = torch.cat((rnn_output, context), 1)\n",
    "        concat_output = torch.tanh(self.concat(concat_input))\n",
    "        # Predict next word using Luong eq. 6\n",
    "        output = self.out(concat_output)\n",
    "        output = F.softmax(output, dim=1)\n",
    "        # Return output and final hidden state\n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "id": "wb3y_lrEPN7V"
   },
   "outputs": [],
   "source": [
    "def maskNLLLoss(inp, target, mask):\n",
    "    nTotal = mask.sum()\n",
    "    \n",
    "    crossEntropy = -torch.log(torch.gather(inp, 1, target.view(-1, 1)).squeeze(1))\n",
    "    loss = crossEntropy.masked_select(mask).mean()\n",
    "    loss = loss.to(device)\n",
    "    return loss, nTotal.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ol8uBgxI1Ew_"
   },
   "source": [
    "## Train Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "id": "g22t-7cdPRc4"
   },
   "outputs": [],
   "source": [
    "def train(input_variable, lengths, target_variable, mask, max_target_len, encoder, decoder, embedding,\n",
    "          encoder_optimizer, decoder_optimizer, batch_size, clip, max_length=MAX_LENGTH):\n",
    "\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "\n",
    "    input_variable = input_variable.to(device)\n",
    "    lengths = lengths.to(device)\n",
    "    target_variable = target_variable.to(device)\n",
    "    mask = mask.to(device)\n",
    "\n",
    "    loss = 0\n",
    "    print_losses = []\n",
    "    n_totals = 0\n",
    "\n",
    "    encoder_outputs, encoder_hidden = encoder(input_variable, lengths)\n",
    "\n",
    "    decoder_input = torch.LongTensor([[SOS_token for _ in range(batch_size)]])\n",
    "    decoder_input = decoder_input.to(device)\n",
    "\n",
    "    decoder_hidden = encoder_hidden[:decoder.n_layers]\n",
    "\n",
    "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
    "\n",
    "    if use_teacher_forcing:\n",
    "        for t in range(max_target_len):\n",
    "            decoder_output, decoder_hidden = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs\n",
    "            )\n",
    "            decoder_input = target_variable[t].view(1, -1)\n",
    "            mask_loss, nTotal = maskNLLLoss(decoder_output, target_variable[t], mask[t])\n",
    "            loss += mask_loss\n",
    "            print_losses.append(mask_loss.item() * nTotal)\n",
    "            n_totals += nTotal\n",
    "    else:\n",
    "        for t in range(max_target_len):\n",
    "            decoder_output, decoder_hidden = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs\n",
    "            )\n",
    "            _, topi = decoder_output.topk(1)\n",
    "            decoder_input = torch.LongTensor([[topi[i][0] for i in range(batch_size)]])\n",
    "            decoder_input = decoder_input.to(device)\n",
    "            \n",
    "            mask_loss, nTotal = maskNLLLoss(decoder_output, target_variable[t], mask[t])\n",
    "            loss += mask_loss\n",
    "            print_losses.append(mask_loss.item() * nTotal)\n",
    "            n_totals += nTotal\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    _ = torch.nn.utils.clip_grad_norm_(encoder.parameters(), clip)\n",
    "    _ = torch.nn.utils.clip_grad_norm_(decoder.parameters(), clip)\n",
    "\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return sum(print_losses) / n_totals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'cb_model'\n",
    "attn_model = 'dot'\n",
    "#attn_model = 'general'\n",
    "#attn_model = 'concat'\n",
    "hidden_size = 500\n",
    "encoder_n_layers = 2\n",
    "decoder_n_layers = 2\n",
    "dropout = 0.1\n",
    "batch_size = 64\n",
    "\n",
    "embedding = nn.Embedding(voc.num_words, hidden_size)\n",
    "\n",
    "encoder = EncoderRNN(hidden_size, embedding, encoder_n_layers, dropout).to(device)\n",
    "decoder = LuongAttnDecoderRNN(attn_model, embedding, hidden_size, voc.num_words, decoder_n_layers, dropout).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "clip = 50.0\n",
    "teacher_forcing_ratio = 1.0\n",
    "learning_rate = 0.0001\n",
    "decoder_learning_ratio = 5.0\n",
    "n_iteration = 1000\n",
    "print_every = 100\n",
    "\n",
    "encoder.train()\n",
    "decoder.train()\n",
    "\n",
    "encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n",
    "decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate * decoder_learning_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "id": "XqP-B7dKPVUV",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mark/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:5: UserWarning: masked_select received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead. (Triggered internally at  ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:733.)\n",
      "  \"\"\"\n",
      "/Users/mark/opt/anaconda3/lib/python3.7/site-packages/torch/autograd/__init__.py:132: UserWarning: masked_scatter_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead. (Triggered internally at  ../aten/src/ATen/native/LegacyDefinitions.cpp:20.)\n",
      "  allow_unreachable=True)  # allow_unreachable flag\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 10; Percent complete: 0.2%; Average loss: 6.1870\n",
      "Iteration: 20; Percent complete: 0.5%; Average loss: 5.4768\n",
      "Iteration: 30; Percent complete: 0.8%; Average loss: 5.2092\n",
      "Iteration: 40; Percent complete: 1.0%; Average loss: 4.8273\n",
      "Iteration: 50; Percent complete: 1.2%; Average loss: 4.2821\n",
      "Iteration: 60; Percent complete: 1.5%; Average loss: 3.7214\n",
      "Iteration: 70; Percent complete: 1.8%; Average loss: 3.1810\n",
      "Iteration: 80; Percent complete: 2.0%; Average loss: 2.6134\n",
      "Iteration: 90; Percent complete: 2.2%; Average loss: 2.0582\n",
      "Iteration: 100; Percent complete: 2.5%; Average loss: 1.6542\n",
      "Iteration: 110; Percent complete: 2.8%; Average loss: 1.2827\n",
      "Iteration: 120; Percent complete: 3.0%; Average loss: 0.9944\n",
      "Iteration: 130; Percent complete: 3.2%; Average loss: 0.7691\n",
      "Iteration: 140; Percent complete: 3.5%; Average loss: 0.5878\n",
      "Iteration: 150; Percent complete: 3.8%; Average loss: 0.4466\n",
      "Iteration: 160; Percent complete: 4.0%; Average loss: 0.3599\n",
      "Iteration: 170; Percent complete: 4.2%; Average loss: 0.2885\n",
      "Iteration: 180; Percent complete: 4.5%; Average loss: 0.2316\n",
      "Iteration: 190; Percent complete: 4.8%; Average loss: 0.1965\n",
      "Iteration: 200; Percent complete: 5.0%; Average loss: 0.1721\n",
      "Iteration: 210; Percent complete: 5.2%; Average loss: 0.1431\n",
      "Iteration: 220; Percent complete: 5.5%; Average loss: 0.1292\n",
      "Iteration: 230; Percent complete: 5.8%; Average loss: 0.1174\n",
      "Iteration: 240; Percent complete: 6.0%; Average loss: 0.1061\n",
      "Iteration: 250; Percent complete: 6.2%; Average loss: 0.0979\n",
      "Iteration: 260; Percent complete: 6.5%; Average loss: 0.0915\n",
      "Iteration: 270; Percent complete: 6.8%; Average loss: 0.0850\n",
      "Iteration: 280; Percent complete: 7.0%; Average loss: 0.0794\n",
      "Iteration: 290; Percent complete: 7.2%; Average loss: 0.0755\n",
      "Iteration: 300; Percent complete: 7.5%; Average loss: 0.0732\n",
      "Iteration: 310; Percent complete: 7.8%; Average loss: 0.0711\n",
      "Iteration: 320; Percent complete: 8.0%; Average loss: 0.0688\n",
      "Iteration: 330; Percent complete: 8.2%; Average loss: 0.0655\n",
      "Iteration: 340; Percent complete: 8.5%; Average loss: 0.0636\n",
      "Iteration: 350; Percent complete: 8.8%; Average loss: 0.0626\n",
      "Iteration: 360; Percent complete: 9.0%; Average loss: 0.0577\n",
      "Iteration: 370; Percent complete: 9.2%; Average loss: 0.0617\n",
      "Iteration: 380; Percent complete: 9.5%; Average loss: 0.0600\n",
      "Iteration: 390; Percent complete: 9.8%; Average loss: 0.0558\n",
      "Iteration: 400; Percent complete: 10.0%; Average loss: 0.0525\n",
      "Iteration: 410; Percent complete: 10.2%; Average loss: 0.0539\n",
      "Iteration: 420; Percent complete: 10.5%; Average loss: 0.0476\n",
      "Iteration: 430; Percent complete: 10.8%; Average loss: 0.0488\n",
      "Iteration: 440; Percent complete: 11.0%; Average loss: 0.0493\n",
      "Iteration: 450; Percent complete: 11.2%; Average loss: 0.0511\n",
      "Iteration: 460; Percent complete: 11.5%; Average loss: 0.0496\n",
      "Iteration: 470; Percent complete: 11.8%; Average loss: 0.0471\n",
      "Iteration: 480; Percent complete: 12.0%; Average loss: 0.0477\n",
      "Iteration: 490; Percent complete: 12.2%; Average loss: 0.0441\n",
      "Iteration: 500; Percent complete: 12.5%; Average loss: 0.0454\n",
      "Iteration: 510; Percent complete: 12.8%; Average loss: 0.0450\n",
      "Iteration: 520; Percent complete: 13.0%; Average loss: 0.0457\n",
      "Iteration: 530; Percent complete: 13.2%; Average loss: 0.0444\n",
      "Iteration: 540; Percent complete: 13.5%; Average loss: 0.0427\n",
      "Iteration: 550; Percent complete: 13.8%; Average loss: 0.0449\n",
      "Iteration: 560; Percent complete: 14.0%; Average loss: 0.0428\n",
      "Iteration: 570; Percent complete: 14.2%; Average loss: 0.0421\n",
      "Iteration: 580; Percent complete: 14.5%; Average loss: 0.0429\n",
      "Iteration: 590; Percent complete: 14.8%; Average loss: 0.0398\n",
      "Iteration: 600; Percent complete: 15.0%; Average loss: 0.0432\n",
      "Iteration: 610; Percent complete: 15.2%; Average loss: 0.0403\n",
      "Iteration: 620; Percent complete: 15.5%; Average loss: 0.0414\n",
      "Iteration: 630; Percent complete: 15.8%; Average loss: 0.0428\n",
      "Iteration: 640; Percent complete: 16.0%; Average loss: 0.0411\n",
      "Iteration: 650; Percent complete: 16.2%; Average loss: 0.0385\n",
      "Iteration: 660; Percent complete: 16.5%; Average loss: 0.0425\n",
      "Iteration: 670; Percent complete: 16.8%; Average loss: 0.0428\n",
      "Iteration: 680; Percent complete: 17.0%; Average loss: 0.0420\n",
      "Iteration: 690; Percent complete: 17.2%; Average loss: 0.0413\n",
      "Iteration: 700; Percent complete: 17.5%; Average loss: 0.0428\n",
      "Iteration: 710; Percent complete: 17.8%; Average loss: 0.0398\n",
      "Iteration: 720; Percent complete: 18.0%; Average loss: 0.0410\n",
      "Iteration: 730; Percent complete: 18.2%; Average loss: 0.0417\n",
      "Iteration: 740; Percent complete: 18.5%; Average loss: 0.0395\n",
      "Iteration: 750; Percent complete: 18.8%; Average loss: 0.0394\n",
      "Iteration: 760; Percent complete: 19.0%; Average loss: 0.0428\n",
      "Iteration: 770; Percent complete: 19.2%; Average loss: 0.0404\n",
      "Iteration: 780; Percent complete: 19.5%; Average loss: 0.0378\n",
      "Iteration: 790; Percent complete: 19.8%; Average loss: 0.0390\n",
      "Iteration: 800; Percent complete: 20.0%; Average loss: 0.0397\n",
      "Iteration: 810; Percent complete: 20.2%; Average loss: 0.0389\n",
      "Iteration: 820; Percent complete: 20.5%; Average loss: 0.0410\n",
      "Iteration: 830; Percent complete: 20.8%; Average loss: 0.0390\n",
      "Iteration: 840; Percent complete: 21.0%; Average loss: 0.0380\n",
      "Iteration: 850; Percent complete: 21.2%; Average loss: 0.0392\n",
      "Iteration: 860; Percent complete: 21.5%; Average loss: 0.0396\n",
      "Iteration: 870; Percent complete: 21.8%; Average loss: 0.0387\n",
      "Iteration: 880; Percent complete: 22.0%; Average loss: 0.0402\n",
      "Iteration: 890; Percent complete: 22.2%; Average loss: 0.0405\n",
      "Iteration: 900; Percent complete: 22.5%; Average loss: 0.0407\n",
      "Iteration: 910; Percent complete: 22.8%; Average loss: 0.0384\n",
      "Iteration: 920; Percent complete: 23.0%; Average loss: 0.0384\n",
      "Iteration: 930; Percent complete: 23.2%; Average loss: 0.0434\n",
      "Iteration: 940; Percent complete: 23.5%; Average loss: 0.0421\n",
      "Iteration: 950; Percent complete: 23.8%; Average loss: 0.0402\n",
      "Iteration: 960; Percent complete: 24.0%; Average loss: 0.0384\n",
      "Iteration: 970; Percent complete: 24.2%; Average loss: 0.0377\n",
      "Iteration: 980; Percent complete: 24.5%; Average loss: 0.0397\n",
      "Iteration: 990; Percent complete: 24.8%; Average loss: 0.0379\n",
      "Iteration: 1000; Percent complete: 25.0%; Average loss: 0.0412\n",
      "Iteration: 1010; Percent complete: 25.2%; Average loss: 0.0380\n",
      "Iteration: 1020; Percent complete: 25.5%; Average loss: 0.0373\n",
      "Iteration: 1030; Percent complete: 25.8%; Average loss: 0.0405\n",
      "Iteration: 1040; Percent complete: 26.0%; Average loss: 0.0382\n",
      "Iteration: 1050; Percent complete: 26.2%; Average loss: 0.0387\n",
      "Iteration: 1060; Percent complete: 26.5%; Average loss: 0.0373\n",
      "Iteration: 1070; Percent complete: 26.8%; Average loss: 0.0377\n",
      "Iteration: 1080; Percent complete: 27.0%; Average loss: 0.0363\n",
      "Iteration: 1090; Percent complete: 27.3%; Average loss: 0.0380\n",
      "Iteration: 1100; Percent complete: 27.5%; Average loss: 0.0367\n",
      "Iteration: 1110; Percent complete: 27.8%; Average loss: 0.0399\n",
      "Iteration: 1120; Percent complete: 28.0%; Average loss: 0.0345\n",
      "Iteration: 1130; Percent complete: 28.2%; Average loss: 0.0380\n",
      "Iteration: 1140; Percent complete: 28.5%; Average loss: 0.0376\n",
      "Iteration: 1150; Percent complete: 28.7%; Average loss: 0.0354\n",
      "Iteration: 1160; Percent complete: 29.0%; Average loss: 0.0358\n",
      "Iteration: 1170; Percent complete: 29.2%; Average loss: 0.0364\n",
      "Iteration: 1180; Percent complete: 29.5%; Average loss: 0.0370\n",
      "Iteration: 1190; Percent complete: 29.8%; Average loss: 0.0365\n",
      "Iteration: 1200; Percent complete: 30.0%; Average loss: 0.0355\n",
      "Iteration: 1210; Percent complete: 30.2%; Average loss: 0.0347\n",
      "Iteration: 1220; Percent complete: 30.5%; Average loss: 0.0365\n",
      "Iteration: 1230; Percent complete: 30.8%; Average loss: 0.0338\n",
      "Iteration: 1240; Percent complete: 31.0%; Average loss: 0.0350\n",
      "Iteration: 1250; Percent complete: 31.2%; Average loss: 0.0348\n",
      "Iteration: 1260; Percent complete: 31.5%; Average loss: 0.0378\n",
      "Iteration: 1270; Percent complete: 31.8%; Average loss: 0.0357\n",
      "Iteration: 1280; Percent complete: 32.0%; Average loss: 0.0345\n",
      "Iteration: 1290; Percent complete: 32.2%; Average loss: 0.0336\n",
      "Iteration: 1300; Percent complete: 32.5%; Average loss: 0.0341\n",
      "Iteration: 1310; Percent complete: 32.8%; Average loss: 0.0354\n",
      "Iteration: 1320; Percent complete: 33.0%; Average loss: 0.0371\n",
      "Iteration: 1330; Percent complete: 33.2%; Average loss: 0.0346\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1340; Percent complete: 33.5%; Average loss: 0.0386\n",
      "Iteration: 1350; Percent complete: 33.8%; Average loss: 0.0383\n",
      "Iteration: 1360; Percent complete: 34.0%; Average loss: 0.0381\n",
      "Iteration: 1370; Percent complete: 34.2%; Average loss: 0.0371\n",
      "Iteration: 1380; Percent complete: 34.5%; Average loss: 0.0348\n",
      "Iteration: 1390; Percent complete: 34.8%; Average loss: 0.0364\n",
      "Iteration: 1400; Percent complete: 35.0%; Average loss: 0.0367\n",
      "Iteration: 1410; Percent complete: 35.2%; Average loss: 0.0346\n",
      "Iteration: 1420; Percent complete: 35.5%; Average loss: 0.0368\n",
      "Iteration: 1430; Percent complete: 35.8%; Average loss: 0.0361\n",
      "Iteration: 1440; Percent complete: 36.0%; Average loss: 0.0341\n",
      "Iteration: 1450; Percent complete: 36.2%; Average loss: 0.0354\n",
      "Iteration: 1460; Percent complete: 36.5%; Average loss: 0.0345\n",
      "Iteration: 1470; Percent complete: 36.8%; Average loss: 0.0364\n",
      "Iteration: 1480; Percent complete: 37.0%; Average loss: 0.0348\n",
      "Iteration: 1490; Percent complete: 37.2%; Average loss: 0.0362\n",
      "Iteration: 1500; Percent complete: 37.5%; Average loss: 0.0358\n",
      "Iteration: 1510; Percent complete: 37.8%; Average loss: 0.0365\n",
      "Iteration: 1520; Percent complete: 38.0%; Average loss: 0.0358\n",
      "Iteration: 1530; Percent complete: 38.2%; Average loss: 0.0348\n",
      "Iteration: 1540; Percent complete: 38.5%; Average loss: 0.0378\n",
      "Iteration: 1550; Percent complete: 38.8%; Average loss: 0.0342\n",
      "Iteration: 1560; Percent complete: 39.0%; Average loss: 0.0357\n",
      "Iteration: 1570; Percent complete: 39.2%; Average loss: 0.0350\n",
      "Iteration: 1580; Percent complete: 39.5%; Average loss: 0.0376\n",
      "Iteration: 1590; Percent complete: 39.8%; Average loss: 0.0357\n",
      "Iteration: 1600; Percent complete: 40.0%; Average loss: 0.0345\n",
      "Iteration: 1610; Percent complete: 40.2%; Average loss: 0.0346\n",
      "Iteration: 1620; Percent complete: 40.5%; Average loss: 0.0384\n",
      "Iteration: 1630; Percent complete: 40.8%; Average loss: 0.0349\n",
      "Iteration: 1640; Percent complete: 41.0%; Average loss: 0.0356\n",
      "Iteration: 1650; Percent complete: 41.2%; Average loss: 0.0333\n",
      "Iteration: 1660; Percent complete: 41.5%; Average loss: 0.0328\n",
      "Iteration: 1670; Percent complete: 41.8%; Average loss: 0.0336\n",
      "Iteration: 1680; Percent complete: 42.0%; Average loss: 0.0346\n",
      "Iteration: 1690; Percent complete: 42.2%; Average loss: 0.0334\n",
      "Iteration: 1700; Percent complete: 42.5%; Average loss: 0.0350\n",
      "Iteration: 1710; Percent complete: 42.8%; Average loss: 0.0336\n",
      "Iteration: 1720; Percent complete: 43.0%; Average loss: 0.0335\n",
      "Iteration: 1730; Percent complete: 43.2%; Average loss: 0.0355\n",
      "Iteration: 1740; Percent complete: 43.5%; Average loss: 0.0355\n",
      "Iteration: 1750; Percent complete: 43.8%; Average loss: 0.0341\n",
      "Iteration: 1760; Percent complete: 44.0%; Average loss: 0.0342\n",
      "Iteration: 1770; Percent complete: 44.2%; Average loss: 0.0343\n",
      "Iteration: 1780; Percent complete: 44.5%; Average loss: 0.0352\n",
      "Iteration: 1790; Percent complete: 44.8%; Average loss: 0.0329\n",
      "Iteration: 1800; Percent complete: 45.0%; Average loss: 0.0340\n",
      "Iteration: 1810; Percent complete: 45.2%; Average loss: 0.0343\n",
      "Iteration: 1820; Percent complete: 45.5%; Average loss: 0.0319\n",
      "Iteration: 1830; Percent complete: 45.8%; Average loss: 0.0342\n",
      "Iteration: 1840; Percent complete: 46.0%; Average loss: 0.0323\n",
      "Iteration: 1850; Percent complete: 46.2%; Average loss: 0.0348\n",
      "Iteration: 1860; Percent complete: 46.5%; Average loss: 0.0335\n",
      "Iteration: 1870; Percent complete: 46.8%; Average loss: 0.0362\n",
      "Iteration: 1880; Percent complete: 47.0%; Average loss: 0.0329\n",
      "Iteration: 1890; Percent complete: 47.2%; Average loss: 0.0330\n",
      "Iteration: 1900; Percent complete: 47.5%; Average loss: 0.0326\n",
      "Iteration: 1910; Percent complete: 47.8%; Average loss: 0.0332\n",
      "Iteration: 1920; Percent complete: 48.0%; Average loss: 0.0337\n",
      "Iteration: 1930; Percent complete: 48.2%; Average loss: 0.0338\n",
      "Iteration: 1940; Percent complete: 48.5%; Average loss: 0.0352\n",
      "Iteration: 1950; Percent complete: 48.8%; Average loss: 0.0349\n",
      "Iteration: 1960; Percent complete: 49.0%; Average loss: 0.0324\n",
      "Iteration: 1970; Percent complete: 49.2%; Average loss: 0.0345\n",
      "Iteration: 1980; Percent complete: 49.5%; Average loss: 0.0314\n",
      "Iteration: 1990; Percent complete: 49.8%; Average loss: 0.0343\n",
      "Iteration: 2000; Percent complete: 50.0%; Average loss: 0.0352\n",
      "Iteration: 2010; Percent complete: 50.2%; Average loss: 0.0331\n",
      "Iteration: 2020; Percent complete: 50.5%; Average loss: 0.0342\n",
      "Iteration: 2030; Percent complete: 50.7%; Average loss: 0.0359\n",
      "Iteration: 2040; Percent complete: 51.0%; Average loss: 0.0330\n",
      "Iteration: 2050; Percent complete: 51.2%; Average loss: 0.0333\n",
      "Iteration: 2060; Percent complete: 51.5%; Average loss: 0.0328\n",
      "Iteration: 2070; Percent complete: 51.7%; Average loss: 0.0337\n",
      "Iteration: 2080; Percent complete: 52.0%; Average loss: 0.0357\n",
      "Iteration: 2090; Percent complete: 52.2%; Average loss: 0.0333\n",
      "Iteration: 2100; Percent complete: 52.5%; Average loss: 0.0344\n",
      "Iteration: 2110; Percent complete: 52.8%; Average loss: 0.0331\n",
      "Iteration: 2120; Percent complete: 53.0%; Average loss: 0.0344\n",
      "Iteration: 2130; Percent complete: 53.2%; Average loss: 0.0348\n",
      "Iteration: 2140; Percent complete: 53.5%; Average loss: 0.0336\n",
      "Iteration: 2150; Percent complete: 53.8%; Average loss: 0.0356\n",
      "Iteration: 2160; Percent complete: 54.0%; Average loss: 0.0335\n",
      "Iteration: 2170; Percent complete: 54.2%; Average loss: 0.0360\n",
      "Iteration: 2180; Percent complete: 54.5%; Average loss: 0.0335\n",
      "Iteration: 2190; Percent complete: 54.8%; Average loss: 0.0346\n",
      "Iteration: 2200; Percent complete: 55.0%; Average loss: 0.0346\n",
      "Iteration: 2210; Percent complete: 55.2%; Average loss: 0.0349\n",
      "Iteration: 2220; Percent complete: 55.5%; Average loss: 0.0341\n",
      "Iteration: 2230; Percent complete: 55.8%; Average loss: 0.0344\n",
      "Iteration: 2240; Percent complete: 56.0%; Average loss: 0.0347\n",
      "Iteration: 2250; Percent complete: 56.2%; Average loss: 0.0335\n",
      "Iteration: 2260; Percent complete: 56.5%; Average loss: 0.0352\n",
      "Iteration: 2270; Percent complete: 56.8%; Average loss: 0.0344\n",
      "Iteration: 2280; Percent complete: 57.0%; Average loss: 0.0340\n",
      "Iteration: 2290; Percent complete: 57.2%; Average loss: 0.0334\n",
      "Iteration: 2300; Percent complete: 57.5%; Average loss: 0.0347\n",
      "Iteration: 2310; Percent complete: 57.8%; Average loss: 0.0324\n",
      "Iteration: 2320; Percent complete: 58.0%; Average loss: 0.0322\n",
      "Iteration: 2330; Percent complete: 58.2%; Average loss: 0.0370\n",
      "Iteration: 2340; Percent complete: 58.5%; Average loss: 0.0321\n",
      "Iteration: 2350; Percent complete: 58.8%; Average loss: 0.0331\n",
      "Iteration: 2360; Percent complete: 59.0%; Average loss: 0.0320\n",
      "Iteration: 2370; Percent complete: 59.2%; Average loss: 0.0333\n",
      "Iteration: 2380; Percent complete: 59.5%; Average loss: 0.0360\n",
      "Iteration: 2390; Percent complete: 59.8%; Average loss: 0.0341\n",
      "Iteration: 2400; Percent complete: 60.0%; Average loss: 0.0338\n",
      "Iteration: 2410; Percent complete: 60.2%; Average loss: 0.0324\n",
      "Iteration: 2420; Percent complete: 60.5%; Average loss: 0.0310\n",
      "Iteration: 2430; Percent complete: 60.8%; Average loss: 0.0342\n",
      "Iteration: 2440; Percent complete: 61.0%; Average loss: 0.0329\n",
      "Iteration: 2450; Percent complete: 61.3%; Average loss: 0.0322\n",
      "Iteration: 2460; Percent complete: 61.5%; Average loss: 0.0347\n",
      "Iteration: 2470; Percent complete: 61.8%; Average loss: 0.0329\n",
      "Iteration: 2480; Percent complete: 62.0%; Average loss: 0.0341\n",
      "Iteration: 2490; Percent complete: 62.3%; Average loss: 0.0342\n",
      "Iteration: 2500; Percent complete: 62.5%; Average loss: 0.0331\n",
      "Iteration: 2510; Percent complete: 62.7%; Average loss: 0.0314\n",
      "Iteration: 2520; Percent complete: 63.0%; Average loss: 0.0323\n",
      "Iteration: 2530; Percent complete: 63.2%; Average loss: 0.0327\n",
      "Iteration: 2540; Percent complete: 63.5%; Average loss: 0.0348\n",
      "Iteration: 2550; Percent complete: 63.7%; Average loss: 0.0321\n",
      "Iteration: 2560; Percent complete: 64.0%; Average loss: 0.0341\n",
      "Iteration: 2570; Percent complete: 64.2%; Average loss: 0.0325\n",
      "Iteration: 2580; Percent complete: 64.5%; Average loss: 0.0323\n",
      "Iteration: 2590; Percent complete: 64.8%; Average loss: 0.0333\n",
      "Iteration: 2600; Percent complete: 65.0%; Average loss: 0.0340\n",
      "Iteration: 2610; Percent complete: 65.2%; Average loss: 0.0314\n",
      "Iteration: 2620; Percent complete: 65.5%; Average loss: 0.0339\n",
      "Iteration: 2630; Percent complete: 65.8%; Average loss: 0.0340\n",
      "Iteration: 2640; Percent complete: 66.0%; Average loss: 0.0325\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 2650; Percent complete: 66.2%; Average loss: 0.0325\n",
      "Iteration: 2660; Percent complete: 66.5%; Average loss: 0.0341\n",
      "Iteration: 2670; Percent complete: 66.8%; Average loss: 0.0332\n",
      "Iteration: 2680; Percent complete: 67.0%; Average loss: 0.0339\n",
      "Iteration: 2690; Percent complete: 67.2%; Average loss: 0.0343\n",
      "Iteration: 2700; Percent complete: 67.5%; Average loss: 0.0326\n",
      "Iteration: 2710; Percent complete: 67.8%; Average loss: 0.0342\n",
      "Iteration: 2720; Percent complete: 68.0%; Average loss: 0.0347\n",
      "Iteration: 2730; Percent complete: 68.2%; Average loss: 0.0324\n",
      "Iteration: 2740; Percent complete: 68.5%; Average loss: 0.0339\n",
      "Iteration: 2750; Percent complete: 68.8%; Average loss: 0.0333\n",
      "Iteration: 2760; Percent complete: 69.0%; Average loss: 0.0330\n",
      "Iteration: 2770; Percent complete: 69.2%; Average loss: 0.0331\n",
      "Iteration: 2780; Percent complete: 69.5%; Average loss: 0.0331\n",
      "Iteration: 2790; Percent complete: 69.8%; Average loss: 0.0332\n",
      "Iteration: 2800; Percent complete: 70.0%; Average loss: 0.0347\n",
      "Iteration: 2810; Percent complete: 70.2%; Average loss: 0.0350\n",
      "Iteration: 2820; Percent complete: 70.5%; Average loss: 0.0333\n",
      "Iteration: 2830; Percent complete: 70.8%; Average loss: 0.0338\n",
      "Iteration: 2840; Percent complete: 71.0%; Average loss: 0.0317\n",
      "Iteration: 2850; Percent complete: 71.2%; Average loss: 0.0347\n",
      "Iteration: 2860; Percent complete: 71.5%; Average loss: 0.0337\n",
      "Iteration: 2870; Percent complete: 71.8%; Average loss: 0.0326\n",
      "Iteration: 2880; Percent complete: 72.0%; Average loss: 0.0338\n",
      "Iteration: 2890; Percent complete: 72.2%; Average loss: 0.0332\n",
      "Iteration: 2900; Percent complete: 72.5%; Average loss: 0.0326\n",
      "Iteration: 2910; Percent complete: 72.8%; Average loss: 0.0341\n",
      "Iteration: 2920; Percent complete: 73.0%; Average loss: 0.0345\n",
      "Iteration: 2930; Percent complete: 73.2%; Average loss: 0.0338\n",
      "Iteration: 2940; Percent complete: 73.5%; Average loss: 0.0323\n",
      "Iteration: 2950; Percent complete: 73.8%; Average loss: 0.0329\n",
      "Iteration: 2960; Percent complete: 74.0%; Average loss: 0.0312\n",
      "Iteration: 2970; Percent complete: 74.2%; Average loss: 0.0336\n",
      "Iteration: 2980; Percent complete: 74.5%; Average loss: 0.0333\n",
      "Iteration: 2990; Percent complete: 74.8%; Average loss: 0.0322\n",
      "Iteration: 3000; Percent complete: 75.0%; Average loss: 0.0331\n",
      "Iteration: 3010; Percent complete: 75.2%; Average loss: 0.0333\n",
      "Iteration: 3020; Percent complete: 75.5%; Average loss: 0.0303\n",
      "Iteration: 3030; Percent complete: 75.8%; Average loss: 0.0340\n",
      "Iteration: 3040; Percent complete: 76.0%; Average loss: 0.0343\n",
      "Iteration: 3050; Percent complete: 76.2%; Average loss: 0.0332\n",
      "Iteration: 3060; Percent complete: 76.5%; Average loss: 0.0318\n",
      "Iteration: 3070; Percent complete: 76.8%; Average loss: 0.0331\n",
      "Iteration: 3080; Percent complete: 77.0%; Average loss: 0.0322\n",
      "Iteration: 3090; Percent complete: 77.2%; Average loss: 0.0327\n",
      "Iteration: 3100; Percent complete: 77.5%; Average loss: 0.0337\n",
      "Iteration: 3110; Percent complete: 77.8%; Average loss: 0.0312\n",
      "Iteration: 3120; Percent complete: 78.0%; Average loss: 0.0329\n",
      "Iteration: 3130; Percent complete: 78.2%; Average loss: 0.0319\n",
      "Iteration: 3140; Percent complete: 78.5%; Average loss: 0.0319\n",
      "Iteration: 3150; Percent complete: 78.8%; Average loss: 0.0329\n",
      "Iteration: 3160; Percent complete: 79.0%; Average loss: 0.0347\n",
      "Iteration: 3170; Percent complete: 79.2%; Average loss: 0.0322\n",
      "Iteration: 3180; Percent complete: 79.5%; Average loss: 0.0341\n",
      "Iteration: 3190; Percent complete: 79.8%; Average loss: 0.0341\n",
      "Iteration: 3200; Percent complete: 80.0%; Average loss: 0.0339\n",
      "Iteration: 3210; Percent complete: 80.2%; Average loss: 0.0347\n",
      "Iteration: 3220; Percent complete: 80.5%; Average loss: 0.0330\n",
      "Iteration: 3230; Percent complete: 80.8%; Average loss: 0.0349\n",
      "Iteration: 3240; Percent complete: 81.0%; Average loss: 0.0338\n",
      "Iteration: 3250; Percent complete: 81.2%; Average loss: 0.0327\n",
      "Iteration: 3260; Percent complete: 81.5%; Average loss: 0.0332\n",
      "Iteration: 3270; Percent complete: 81.8%; Average loss: 0.0324\n",
      "Iteration: 3280; Percent complete: 82.0%; Average loss: 0.0334\n",
      "Iteration: 3290; Percent complete: 82.2%; Average loss: 0.0353\n",
      "Iteration: 3300; Percent complete: 82.5%; Average loss: 0.0320\n",
      "Iteration: 3310; Percent complete: 82.8%; Average loss: 0.0315\n",
      "Iteration: 3320; Percent complete: 83.0%; Average loss: 0.0342\n",
      "Iteration: 3330; Percent complete: 83.2%; Average loss: 0.0327\n",
      "Iteration: 3340; Percent complete: 83.5%; Average loss: 0.0322\n",
      "Iteration: 3350; Percent complete: 83.8%; Average loss: 0.0327\n",
      "Iteration: 3360; Percent complete: 84.0%; Average loss: 0.0328\n",
      "Iteration: 3370; Percent complete: 84.2%; Average loss: 0.0325\n",
      "Iteration: 3380; Percent complete: 84.5%; Average loss: 0.0318\n",
      "Iteration: 3390; Percent complete: 84.8%; Average loss: 0.0334\n",
      "Iteration: 3400; Percent complete: 85.0%; Average loss: 0.0329\n",
      "Iteration: 3410; Percent complete: 85.2%; Average loss: 0.0324\n",
      "Iteration: 3420; Percent complete: 85.5%; Average loss: 0.0340\n",
      "Iteration: 3430; Percent complete: 85.8%; Average loss: 0.0334\n",
      "Iteration: 3440; Percent complete: 86.0%; Average loss: 0.0340\n",
      "Iteration: 3450; Percent complete: 86.2%; Average loss: 0.0348\n",
      "Iteration: 3460; Percent complete: 86.5%; Average loss: 0.0322\n",
      "Iteration: 3470; Percent complete: 86.8%; Average loss: 0.0317\n",
      "Iteration: 3480; Percent complete: 87.0%; Average loss: 0.0337\n",
      "Iteration: 3490; Percent complete: 87.2%; Average loss: 0.0332\n",
      "Iteration: 3500; Percent complete: 87.5%; Average loss: 0.0352\n",
      "Iteration: 3510; Percent complete: 87.8%; Average loss: 0.0299\n",
      "Iteration: 3520; Percent complete: 88.0%; Average loss: 0.0316\n",
      "Iteration: 3530; Percent complete: 88.2%; Average loss: 0.0330\n",
      "Iteration: 3540; Percent complete: 88.5%; Average loss: 0.0344\n",
      "Iteration: 3550; Percent complete: 88.8%; Average loss: 0.0333\n",
      "Iteration: 3560; Percent complete: 89.0%; Average loss: 0.0332\n",
      "Iteration: 3570; Percent complete: 89.2%; Average loss: 0.0342\n",
      "Iteration: 3580; Percent complete: 89.5%; Average loss: 0.0344\n",
      "Iteration: 3590; Percent complete: 89.8%; Average loss: 0.0324\n",
      "Iteration: 3600; Percent complete: 90.0%; Average loss: 0.0334\n",
      "Iteration: 3610; Percent complete: 90.2%; Average loss: 0.0320\n",
      "Iteration: 3620; Percent complete: 90.5%; Average loss: 0.0319\n",
      "Iteration: 3630; Percent complete: 90.8%; Average loss: 0.0330\n",
      "Iteration: 3640; Percent complete: 91.0%; Average loss: 0.0341\n",
      "Iteration: 3650; Percent complete: 91.2%; Average loss: 0.0359\n",
      "Iteration: 3660; Percent complete: 91.5%; Average loss: 0.0327\n",
      "Iteration: 3670; Percent complete: 91.8%; Average loss: 0.0325\n",
      "Iteration: 3680; Percent complete: 92.0%; Average loss: 0.0335\n",
      "Iteration: 3690; Percent complete: 92.2%; Average loss: 0.0342\n",
      "Iteration: 3700; Percent complete: 92.5%; Average loss: 0.0328\n",
      "Iteration: 3710; Percent complete: 92.8%; Average loss: 0.0311\n",
      "Iteration: 3720; Percent complete: 93.0%; Average loss: 0.0321\n",
      "Iteration: 3730; Percent complete: 93.2%; Average loss: 0.0345\n",
      "Iteration: 3740; Percent complete: 93.5%; Average loss: 0.0321\n",
      "Iteration: 3750; Percent complete: 93.8%; Average loss: 0.0330\n",
      "Iteration: 3760; Percent complete: 94.0%; Average loss: 0.0310\n",
      "Iteration: 3770; Percent complete: 94.2%; Average loss: 0.0337\n",
      "Iteration: 3780; Percent complete: 94.5%; Average loss: 0.0316\n",
      "Iteration: 3790; Percent complete: 94.8%; Average loss: 0.0340\n",
      "Iteration: 3800; Percent complete: 95.0%; Average loss: 0.0333\n",
      "Iteration: 3810; Percent complete: 95.2%; Average loss: 0.0322\n",
      "Iteration: 3820; Percent complete: 95.5%; Average loss: 0.0322\n",
      "Iteration: 3830; Percent complete: 95.8%; Average loss: 0.0346\n",
      "Iteration: 3840; Percent complete: 96.0%; Average loss: 0.0324\n",
      "Iteration: 3850; Percent complete: 96.2%; Average loss: 0.0319\n",
      "Iteration: 3860; Percent complete: 96.5%; Average loss: 0.0346\n",
      "Iteration: 3870; Percent complete: 96.8%; Average loss: 0.0319\n",
      "Iteration: 3880; Percent complete: 97.0%; Average loss: 0.0325\n",
      "Iteration: 3890; Percent complete: 97.2%; Average loss: 0.0325\n",
      "Iteration: 3900; Percent complete: 97.5%; Average loss: 0.0340\n",
      "Iteration: 3910; Percent complete: 97.8%; Average loss: 0.0345\n",
      "Iteration: 3920; Percent complete: 98.0%; Average loss: 0.0330\n",
      "Iteration: 3930; Percent complete: 98.2%; Average loss: 0.0324\n",
      "Iteration: 3940; Percent complete: 98.5%; Average loss: 0.0337\n",
      "Iteration: 3950; Percent complete: 98.8%; Average loss: 0.0337\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 3960; Percent complete: 99.0%; Average loss: 0.0332\n",
      "Iteration: 3970; Percent complete: 99.2%; Average loss: 0.0337\n",
      "Iteration: 3980; Percent complete: 99.5%; Average loss: 0.0313\n",
      "Iteration: 3990; Percent complete: 99.8%; Average loss: 0.0319\n",
      "Iteration: 4000; Percent complete: 100.0%; Average loss: 0.0331\n"
     ]
    }
   ],
   "source": [
    "training_batches = [batch2TrainData(voc, [random.choice(pairs) for _ in range(batch_size)])\n",
    "                      for _ in range(n_iteration)]\n",
    "\n",
    "start_iteration = 1\n",
    "print_loss = 0\n",
    "\n",
    "for iteration in range(start_iteration, n_iteration + 1):\n",
    "    training_batch = training_batches[iteration - 1]\n",
    "\n",
    "    input_variable, lengths, target_variable, mask, max_target_len = training_batch\n",
    "\n",
    "    loss = train(input_variable, lengths, target_variable, mask, max_target_len, encoder,\n",
    "                 decoder, embedding, encoder_optimizer, decoder_optimizer, batch_size, clip)\n",
    "    print_loss += loss\n",
    "\n",
    "    if iteration % print_every == 0:\n",
    "        print_loss_avg = print_loss / print_every\n",
    "        print(\"Iteration: {}; Percent complete: {:.1f}%; Average loss: {:.4f}\".format(iteration, iteration / n_iteration * 100, print_loss_avg))\n",
    "        print_loss = 0\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iWTw6_bQ1Rxi"
   },
   "source": [
    "## Test Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "id": "E48e4bvFPXcl"
   },
   "outputs": [],
   "source": [
    "class GreedySearchDecoder(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(GreedySearchDecoder, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def forward(self, input_seq, input_length, max_length):\n",
    "        encoder_outputs, encoder_hidden = self.encoder(input_seq, input_length)\n",
    "        decoder_hidden = encoder_hidden[:decoder.n_layers]\n",
    "        decoder_input = torch.ones(1, 1, device=device, dtype=torch.long) * SOS_token\n",
    "        all_tokens = torch.zeros([0], device=device, dtype=torch.long)\n",
    "        all_scores = torch.zeros([0], device=device)\n",
    "        for _ in range(max_length):\n",
    "            decoder_output, decoder_hidden = self.decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
    "            decoder_scores, decoder_input = torch.max(decoder_output, dim=1)\n",
    "            all_tokens = torch.cat((all_tokens, decoder_input), dim=0)\n",
    "            all_scores = torch.cat((all_scores, decoder_scores), dim=0)\n",
    "            decoder_input = torch.unsqueeze(decoder_input, 0)\n",
    "        return all_tokens, all_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(searcher, voc, sentence, max_length=MAX_LENGTH):\n",
    "\n",
    "    words = []\n",
    "    indexes_batch = [indexesFromSentence(voc, sentence)]\n",
    "    lengths = torch.tensor([len(indexes) for indexes in indexes_batch])\n",
    "    input_batch = torch.LongTensor(indexes_batch).transpose(0, 1)\n",
    "    input_batch = input_batch.to(device)\n",
    "    lengths = lengths.to(device)\n",
    "    tokens, scores = searcher(input_batch, lengths, max_length)\n",
    "    decoded_words = [voc.index2word[token.item()] for token in tokens]\n",
    "    for word in decoded_words:\n",
    "        if word == 'EOS':\n",
    "            break\n",
    "        elif word != 'PAD':\n",
    "            words.append(word)\n",
    "    words = ' '.join(words)\n",
    "\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Interactive(searcher, voc):\n",
    "    input_sentence = ''\n",
    "    try:\n",
    "        input_sentence = input('Please input what you want to ask about COVID-19 : ')\n",
    "        input_sentence = normalize(input_sentence)\n",
    "        output_words = evaluate(searcher, voc, input_sentence)\n",
    "        print('COVID-19 Bot:\\n', output_words)\n",
    "\n",
    "    except KeyError:\n",
    "        print(\"Please ask again. You can ask \\\"What is coronavirus?\\\", etc. \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "id": "5bxvRUGMPsBE"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please input what you want to ask about COVID-19 : virus\n",
      "COVID-19 Bot:\n",
      " while there are a number of prevention or treatment of covid . they should only be used as directed by a physician to treat a bacterial infection .\n"
     ]
    }
   ],
   "source": [
    "encoder.eval()\n",
    "decoder.eval()\n",
    "\n",
    "searcher = GreedySearchDecoder(encoder, decoder)\n",
    "\n",
    "Interactive(searcher, voc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iY2B1EjFRMhM"
   },
   "source": [
    "## Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({\n",
    "    'en': encoder.state_dict(),\n",
    "    'de': decoder.state_dict(),\n",
    "    'en_opt': encoder_optimizer.state_dict(),\n",
    "    'de_opt': decoder_optimizer.state_dict(),\n",
    "    'voc_dict': voc.__dict__,\n",
    "    'embedding': embedding.state_dict()\n",
    "}, 'savedWeight.tar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "covid-chatbot.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
